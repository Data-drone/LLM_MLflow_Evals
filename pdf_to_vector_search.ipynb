{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF to Databricks Vector Search Pipeline\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Read a PDF document\n",
        "2. Parse and chunk the content\n",
        "3. Generate embeddings\n",
        "4. Index into Databricks Vector Search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed (should be available in DBR 17.3 LTS)\n",
        "%pip install pypdf langchain databricks_langchain databricks-vectorsearch mlflow\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from databricks_langchain import DatabricksEmbeddings\n",
        "\n",
        "# Databricks imports\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File configuration\n",
        "PDF_PATH = \"data/2025-annual-report.pdf\"\n",
        "\n",
        "# Chunking configuration\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Databricks configuration\n",
        "CATALOG_NAME = \"brian_ml_dev\"  # Update with your catalog name\n",
        "SCHEMA_NAME = \"eval_testing\"  # Update with your schema name\n",
        "TABLE_NAME = \"annual_report_chunks\"  # Table to store chunks\n",
        "VECTOR_SEARCH_ENDPOINT = \"one-env-shared-endpoint-13\"  # Update with your endpoint name\n",
        "VECTOR_INDEX_NAME = \"annual_report_index\"  # Name for the vector index\n",
        "\n",
        "# Embedding model configuration\n",
        "EMBEDDING_MODEL = \"databricks-gte-large-en\"  # Databricks Foundation Model for embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Parse PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load PDF document\n",
        "print(f\"Loading PDF from: {PDF_PATH}\")\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(documents)} pages from PDF\")\n",
        "print(f\"\\nFirst page preview (first 500 chars):\\n{documents[0].page_content[:500]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chunk Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split documents into chunks\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks from the document\")\n",
        "print(f\"\\nFirst chunk preview:\\n{chunks[0].page_content[:300]}...\")\n",
        "print(f\"\\nChunk metadata: {chunks[0].metadata}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Data for Vector Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert chunks to a structured format\n",
        "chunk_data = []\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    chunk_data.append({\n",
        "        \"chunk_id\": f\"chunk_{idx}\",\n",
        "        \"text\": chunk.page_content,\n",
        "        \"page\": chunk.metadata.get(\"page\", -1),\n",
        "        \"source\": chunk.metadata.get(\"source\", PDF_PATH)\n",
        "    })\n",
        "\n",
        "# Create pandas DataFrame\n",
        "chunks_df = pd.DataFrame(chunk_data)\n",
        "print(f\"Created DataFrame with {len(chunks_df)} chunks\")\n",
        "chunks_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Delta Table with Chunks\n",
        "\n",
        "**Important:** The Delta table must have Change Data Feed (CDF) enabled for Databricks Vector Search to work. CDF allows the vector index to automatically sync when the source table is updated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(chunks_df)\n",
        "\n",
        "# Define full table name\n",
        "full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
        "\n",
        "# Write to Delta table with Change Data Feed enabled (required for Vector Search)\n",
        "print(f\"Writing chunks to Delta table: {full_table_name}\")\n",
        "spark_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
        "    .saveAsTable(full_table_name)\n",
        "\n",
        "print(f\"Successfully wrote {spark_df.count()} chunks to Delta table\")\n",
        "\n",
        "# Ensure Change Data Feed is enabled (required for Databricks Vector Search)\n",
        "print(\"Enabling Change Data Feed on the table...\")\n",
        "spark.sql(f\"ALTER TABLE {full_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
        "print(\"✓ Change Data Feed enabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify table creation\n",
        "display(spark.sql(f\"SELECT * FROM {full_table_name} LIMIT 5\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Change Data Feed is enabled\n",
        "table_properties = spark.sql(f\"SHOW TBLPROPERTIES {full_table_name}\").collect()\n",
        "cdf_enabled = any(row['key'] == 'delta.enableChangeDataFeed' and row['value'] == 'true' for row in table_properties)\n",
        "print(f\"Change Data Feed enabled: {cdf_enabled}\")\n",
        "\n",
        "if not cdf_enabled:\n",
        "    print(\"⚠ Warning: Change Data Feed is not enabled. Vector Search will not work!\")\n",
        "    print(\"Run: spark.sql(f'ALTER TABLE {full_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Vector Search Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Vector Search client\n",
        "vsc = VectorSearchClient()\n",
        "\n",
        "print(f\"Vector Search Client initialized\")\n",
        "print(f\"\\nAvailable endpoints:\")\n",
        "try:\n",
        "    endpoints_response = vsc.list_endpoints()\n",
        "    \n",
        "    # Handle different response structures\n",
        "    if hasattr(endpoints_response, 'get'):\n",
        "        # If it's a dict-like object\n",
        "        endpoints_list = endpoints_response.get('endpoints', [endpoints_response])\n",
        "    elif hasattr(endpoints_response, '__iter__'):\n",
        "        # If it's iterable\n",
        "        endpoints_list = list(endpoints_response)\n",
        "    else:\n",
        "        endpoints_list = [endpoints_response]\n",
        "    \n",
        "    for endpoint in endpoints_list:\n",
        "        endpoint_name = endpoint.get('name', 'Unknown') if isinstance(endpoint, dict) else getattr(endpoint, 'name', 'Unknown')\n",
        "        endpoint_status = endpoint.get('endpoint_status', {}).get('state', 'Unknown') if isinstance(endpoint, dict) else 'Unknown'\n",
        "        print(f\"  - {endpoint_name} (Status: {endpoint_status})\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not list endpoints - {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or update vector search index\n",
        "# Full index name\n",
        "full_index_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
        "\n",
        "print(f\"Creating vector search index: {full_index_name}\")\n",
        "print(f\"Source table: {full_table_name}\")\n",
        "print(f\"Endpoint: {VECTOR_SEARCH_ENDPOINT}\")\n",
        "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
        "\n",
        "try:\n",
        "    # Create Delta Sync Index (automatically managed embeddings)\n",
        "    index = vsc.create_delta_sync_index(\n",
        "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
        "        index_name=full_index_name,\n",
        "        source_table_name=full_table_name,\n",
        "        pipeline_type=\"TRIGGERED\",  # or \"CONTINUOUS\" for real-time updates\n",
        "        primary_key=\"chunk_id\",\n",
        "        embedding_source_column=\"text\",  # Column to generate embeddings from\n",
        "        embedding_model_endpoint_name=EMBEDDING_MODEL\n",
        "    )\n",
        "    print(f\"\\n✓ Vector search index created successfully!\")\n",
        "    print(f\"Index details: {index}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError creating index: {e}\")\n",
        "    print(f\"\\nIf the index already exists, you can update it or sync it manually.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Wait for Index to be Ready\n",
        "\n",
        "The vector index needs time to compute embeddings for all chunks. This can take several minutes depending on the size of your document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"Waiting for index to be ready...\")\n",
        "max_wait_time = 600  # 10 minutes\n",
        "wait_interval = 10  # Check every 10 seconds\n",
        "elapsed_time = 0\n",
        "\n",
        "while elapsed_time < max_wait_time:\n",
        "    try:\n",
        "        # Get the index object to check its status\n",
        "        index_obj = vsc.get_index(\n",
        "            endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
        "            index_name=full_index_name\n",
        "        )\n",
        "        \n",
        "        # Try to access status in different ways depending on the API version\n",
        "        status = None\n",
        "        if hasattr(index_obj, 'describe'):\n",
        "            # Newer API - call describe() method\n",
        "            index_info = index_obj.describe()\n",
        "            if isinstance(index_info, dict):\n",
        "                status = index_info.get('status', {}).get('detailed_state', 'UNKNOWN')\n",
        "            else:\n",
        "                status = getattr(getattr(index_info, 'status', {}), 'detailed_state', 'UNKNOWN')\n",
        "        elif hasattr(index_obj, 'status'):\n",
        "            # Direct status attribute\n",
        "            status_obj = index_obj.status\n",
        "            if isinstance(status_obj, dict):\n",
        "                status = status_obj.get('detailed_state', 'UNKNOWN')\n",
        "            else:\n",
        "                status = getattr(status_obj, 'detailed_state', 'UNKNOWN')\n",
        "        else:\n",
        "            # Try to get it as a dictionary\n",
        "            if isinstance(index_obj, dict):\n",
        "                status = index_obj.get('status', {}).get('detailed_state', 'UNKNOWN')\n",
        "        \n",
        "        if status is None:\n",
        "            status = 'UNKNOWN'\n",
        "        \n",
        "        print(f\"[{elapsed_time}s] Index status: {status}\")\n",
        "        \n",
        "        if status == \"ONLINE\" or \"ONLINE_NO_PENDING_UPDATE\":\n",
        "            print(f\"\\n✓ Index is ONLINE and ready to use!\")\n",
        "            break\n",
        "        elif status in [\"FAILED\", \"ERROR\"]:\n",
        "            print(f\"\\n✗ Index creation failed with status: {status}\")\n",
        "            print(f\"Details: {index_obj}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"[{elapsed_time}s] Error checking index status: {e}\")\n",
        "        # Continue trying - index might not be available yet\n",
        "    \n",
        "    time.sleep(wait_interval)\n",
        "    elapsed_time += wait_interval\n",
        "\n",
        "if elapsed_time >= max_wait_time:\n",
        "    print(f\"\\n⚠ Timeout: Index did not become ready within {max_wait_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Check Index Status Manually\n",
        "\n",
        "If the automatic waiting fails, you can check the index status manually:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual index status check\n",
        "try:\n",
        "    index_obj = vsc.get_index(\n",
        "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
        "        index_name=full_index_name\n",
        "    )\n",
        "    \n",
        "    # Try calling describe if available\n",
        "    if hasattr(index_obj, 'describe'):\n",
        "        print(index_obj.describe())\n",
        "    else:\n",
        "        print(f\"Index object: {index_obj}\")\n",
        "        print(f\"\\nIndex object type: {type(index_obj)}\")\n",
        "        print(f\"\\nIndex object attributes: {dir(index_obj)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Vector Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test similarity search\n",
        "test_query = \"What are the key financial highlights from the annual report?\"\n",
        "\n",
        "print(f\"Testing vector search with query: '{test_query}'\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "try:\n",
        "    # Get the index object\n",
        "    index = vsc.get_index(\n",
        "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
        "        index_name=full_index_name\n",
        "    )\n",
        "    \n",
        "    # Perform similarity search\n",
        "    results = index.similarity_search(\n",
        "        query_text=test_query,\n",
        "        columns=[\"chunk_id\", \"text\", \"page\", \"source\"],\n",
        "        num_results=3\n",
        "    )\n",
        "    \n",
        "    # Handle results - they might be in different formats\n",
        "    if isinstance(results, dict):\n",
        "        data_array = results.get('result', {}).get('data_array', [])\n",
        "    else:\n",
        "        data_array = getattr(results, 'data_array', [])\n",
        "    \n",
        "    print(f\"\\nTop {len(data_array)} relevant chunks:\\n\")\n",
        "    \n",
        "    for i, result in enumerate(data_array, 1):\n",
        "        print(f\"Result {i}:\")\n",
        "        print(f\"  Chunk ID: {result[0]}\")\n",
        "        print(f\"  Page: {result[2]}\")\n",
        "        print(f\"  Text preview: {result[1][:200]}...\")\n",
        "        if len(result) > 4:\n",
        "            print(f\"  Score: {result[-1]}\")\n",
        "        print()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error during similarity search: {e}\")\n",
        "    print(\"\\nMake sure the index is ONLINE before running similarity search.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PIPELINE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n✓ PDF loaded: {PDF_PATH}\")\n",
        "print(f\"✓ Number of chunks created: {len(chunks)}\")\n",
        "print(f\"✓ Delta table: {full_table_name}\")\n",
        "print(f\"✓ Vector search index: {full_index_name}\")\n",
        "print(f\"✓ Embedding model: {EMBEDDING_MODEL}\")\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"1. Use the vector search index for RAG applications\")\n",
        "print(\"2. Build LangChain/LangGraph agents with this knowledge base\")\n",
        "print(\"3. Log models with MLflow 3.4+ using Model As Code\")\n",
        "print(\"4. Monitor and evaluate your RAG pipeline\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
