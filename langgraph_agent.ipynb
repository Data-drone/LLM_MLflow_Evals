{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph RAG Agent with Databricks Vector Search\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Creating a LangGraph agent with retrieval capabilities\n",
        "2. Testing the agent with sample queries\n",
        "3. Logging the agent as an MLflow model using Model As Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langgraph langchain databricks_agents databricks_langchain databricks-vectorsearch \"mlflow>=3.6\" pandas matplotlib\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import uuid\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "from databricks_langchain import ChatDatabricks\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n",
        "assert mlflow.__version__ >= \"3.6.0\", \"MLflow 3.6+ required for session/user tracking\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector Search configuration (from pdf_to_vector_search notebook)\n",
        "CATALOG_NAME = \"brian_ml_dev\"\n",
        "SCHEMA_NAME = \"eval_testing\"\n",
        "VECTOR_INDEX_NAME = \"annual_report_index\"\n",
        "VECTOR_SEARCH_ENDPOINT = \"one-env-shared-endpoint-13\"\n",
        "\n",
        "# LLM configuration\n",
        "LLM_ENDPOINT = \"databricks-gpt-oss-120b\"\n",
        "\n",
        "# Agent configuration\n",
        "TOP_K_RESULTS = 3\n",
        "MAX_ITERATIONS = 5\n",
        "\n",
        "# MLflow configuration\n",
        "EXPERIMENT_NAME = \"/Users/brian.law@databricks.com/langgraph_rag_agent\"\n",
        "MODEL_NAME = \"langgraph_rag_agent\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Vector Search client\n",
        "vsc = VectorSearchClient()\n",
        "full_index_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
        "\n",
        "# Get vector search index\n",
        "vector_index = vsc.get_index(\n",
        "    endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
        "    index_name=full_index_name\n",
        ")\n",
        "\n",
        "print(f\"✓ Vector Search Index loaded: {full_index_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = ChatDatabricks(\n",
        "    endpoint=LLM_ENDPOINT,\n",
        "    temperature=0.1,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(f\"✓ LLM initialized: {LLM_ENDPOINT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Agent State and Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    \"\"\"State of the agent graph\"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    question: str\n",
        "    context: str\n",
        "    answer: str\n",
        "    iterations: int\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_documents(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieve relevant documents from vector search\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    # Perform similarity search\n",
        "    results = vector_index.similarity_search(\n",
        "        query_text=question,\n",
        "        columns=[\"text\", \"page\"],\n",
        "        num_results=TOP_K_RESULTS\n",
        "    )\n",
        "    \n",
        "    # Extract text from results\n",
        "    if isinstance(results, dict):\n",
        "        data_array = results.get('result', {}).get('data_array', [])\n",
        "    else:\n",
        "        data_array = getattr(results, 'data_array', [])\n",
        "    \n",
        "    # Format context from retrieved documents\n",
        "    context_parts = []\n",
        "    for i, result in enumerate(data_array, 1):\n",
        "        text = result[0] if isinstance(result, (list, tuple)) else result\n",
        "        context_parts.append(f\"Document {i}:\\n{text}\")\n",
        "    \n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"context\": context,\n",
        "        \"messages\": state[\"messages\"] + [SystemMessage(content=f\"Retrieved {len(data_array)} relevant documents\")]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(state: AgentState) -> AgentState:\n",
        "    \"\"\"Generate answer using LLM with retrieved context\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    context = state[\"context\"]\n",
        "    \n",
        "    # Create RAG prompt\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that answers questions based on the provided context. \"\n",
        "                   \"If the answer cannot be found in the context, say so.\"),\n",
        "        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\")\n",
        "    ])\n",
        "    \n",
        "    # Generate response\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"context\": context, \"question\": question})\n",
        "    \n",
        "    answer = response.content if hasattr(response, 'content') else str(response)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"answer\": answer,\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=answer)],\n",
        "        \"iterations\": state.get(\"iterations\", 0) + 1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build LangGraph Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"retrieve\", retrieve_documents)\n",
        "workflow.add_node(\"generate\", generate_answer)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# Add edges\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile the graph\n",
        "agent = workflow.compile()\n",
        "\n",
        "print(\"✓ LangGraph agent created successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Session and user tracking for chat/conversation functionality (MLflow 3.6+)\n",
        "# Generate session and user IDs for tracking conversations\n",
        "session_id = str(uuid.uuid4())\n",
        "user_id = \"test_user_001\"  # In production, use actual user identifier\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"What are the key financial highlights from the annual report?\",\n",
        "    \"What are the main risks mentioned in the report?\",\n",
        "    \"What is the company's growth strategy?\"\n",
        "]\n",
        "\n",
        "print(\"Testing the agent with session/user tracking...\\n\" + \"=\"*80)\n",
        "print(f\"Session ID: {session_id}\")\n",
        "print(f\"User ID: {user_id}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Enable MLflow autologging for LangChain\n",
        "mlflow.langchain.autolog()\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\nTest {i}: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Run the agent\n",
        "    result = agent.invoke({\n",
        "        \"messages\": [HumanMessage(content=question)],\n",
        "        \"question\": question,\n",
        "        \"context\": \"\",\n",
        "        \"answer\": \"\",\n",
        "        \"iterations\": 0\n",
        "    })\n",
        "    \n",
        "    # Add session and user metadata to the current trace (MLflow 3.6+)\n",
        "    mlflow.update_current_trace(\n",
        "        metadata={\n",
        "            \"mlflow.trace.session\": session_id,\n",
        "            \"mlflow.trace.user\": user_id,\n",
        "            \"question_index\": i,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nAnswer: {result['answer']}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create MLflow Model Wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.models.set_model(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n✓ LangGraph RAG agent created\")\n",
        "print(f\"✓ Agent tested with sample queries\")\n",
        "print(f\"✓ Session/user tracking enabled (MLflow 3.6+)\")\n",
        "print(f\"✓ Model set with mlflow.models.set_model()\")\n",
        "print(f\"\\nAgent Configuration:\")\n",
        "print(f\"  - Vector Index: {full_index_name}\")\n",
        "print(f\"  - LLM Endpoint: {LLM_ENDPOINT}\")\n",
        "print(f\"  - Top K Results: {TOP_K_RESULTS}\")\n",
        "print(f\"\\nSession Tracking:\")\n",
        "print(f\"  - Session ID: {session_id}\")\n",
        "print(f\"  - User ID: {user_id}\")\n",
        "print(f\"\\nNext step: Use this notebook code within an MLflow run to log the model\")\n",
        "print(f\"          Then use model_evaluation.ipynb to evaluate it\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
