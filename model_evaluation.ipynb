{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow GenAI Model Evaluation\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Logging the LangGraph RAG agent model with MLflow\n",
        "2. Creating evaluation datasets\n",
        "3. Running mlflow.genai.evaluate with multiple scorers\n",
        "4. Analyzing evaluation results\n",
        "\n",
        "**Prerequisites:** Run langgraph_agent.ipynb first to create the agent code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langgraph langchain databricks_agents databricks_langchain databricks-vectorsearch \"mlflow>=3.6\" pandas matplotlib litellm\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "from mlflow.genai import evaluate\n",
        "\n",
        "# MLflow 3.6+ Evaluation Dataset and Scorer Registration imports\n",
        "import mlflow.genai.datasets\n",
        "from mlflow.genai.datasets import create_dataset, get_dataset\n",
        "from mlflow.genai.scorers import (\n",
        "    Safety, RelevanceToQuery,\n",
        "    ScorerSamplingConfig, scorer, list_scorers, get_scorer\n",
        ")\n",
        "from mlflow.genai.judges import make_judge\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n",
        "assert mlflow.__version__ >= \"3.6.0\", \"MLflow 3.6+ required for dataset/scorer registration\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog configuration\n",
        "CATALOG_NAME = \"brian_ml_dev\"\n",
        "SCHEMA_NAME = \"eval_testing\"\n",
        "UC_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"  # For dataset/table references\n",
        "\n",
        "# MLflow configuration\n",
        "EXPERIMENT_NAME = \"/Users/brian.law@databricks.com/langgraph_rag_agent\"\n",
        "EVAL_EXPERIMENT_NAME = \"/Users/brian.law@databricks.com/langgraph_rag_evaluation\"\n",
        "\n",
        "# Evaluation Dataset configuration (stored in Unity Catalog)\n",
        "EVAL_DATASET_NAME = \"rag_agent_eval_dataset\"\n",
        "EVAL_DATASET_UC_TABLE = f\"{UC_SCHEMA}.{EVAL_DATASET_NAME}\"\n",
        "\n",
        "# Vector Search configuration (from pdf_to_vector_search notebook)\n",
        "VECTOR_INDEX_NAME = \"annual_report_index\"\n",
        "VECTOR_SEARCH_ENDPOINT = \"one-env-shared-endpoint-13\"\n",
        "\n",
        "# LLM configuration\n",
        "LLM_ENDPOINT = \"databricks-gpt-oss-120b\"\n",
        "TOP_K_RESULTS = 3\n",
        "\n",
        "# Judge LLM for evaluation\n",
        "JUDGE_LLM = \"databricks:/databricks-gpt-oss-120b\"\n",
        "\n",
        "# Production monitoring configuration\n",
        "PRODUCTION_SAMPLE_RATE = 0.5  # Sample 50% of traces for scorer monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Log Model with MLflow Model As Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set experiment and log model using Model As Code\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "with mlflow.start_run(run_name=\"langgraph_rag_agent\") as run:\n",
        "    \n",
        "    # Log model using MLflow LangChain with Model As Code\n",
        "    # Just reference the notebook path - no need to instantiate the class!\n",
        "    full_index_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
        "    \n",
        "    mlflow.langchain.log_model(\n",
        "        lc_model=\"langgraph_agent\",  # Reference the notebook directly!\n",
        "        name=\"model\",\n",
        "        input_example=pd.DataFrame({\"question\": [\"What is in the annual report?\"]}),\n",
        "        extra_pip_requirements=[\n",
        "            \"langgraph\",\n",
        "            \"langchain\",\n",
        "            \"databricks_langchain\",\n",
        "            \"databricks-vectorsearch\"\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"vector_index\": full_index_name,\n",
        "        \"vector_endpoint\": VECTOR_SEARCH_ENDPOINT,\n",
        "        \"llm_endpoint\": LLM_ENDPOINT,\n",
        "        \"top_k_results\": TOP_K_RESULTS\n",
        "    })\n",
        "    \n",
        "    model_run_id = run.info.run_id\n",
        "    model_uri = f\"runs:/{model_run_id}/model\"\n",
        "    \n",
        "    print(f\"âœ“ Model logged successfully with MLflow Model As Code!\")\n",
        "    print(f\"Run ID: {model_run_id}\")\n",
        "    print(f\"Model URI: {model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Evaluation Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set experiment before dataset operations (required for MLflow 3.6+)\n",
        "mlflow.set_experiment(EVAL_EXPERIMENT_NAME)\n",
        "\n",
        "# implement a basic get or create command\n",
        "def get_or_create_dataset(uc_table_name: str, **kwargs):\n",
        "    \"\"\"Get existing dataset or create new one if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        # Try to get existing dataset by UC table name\n",
        "        dataset = get_dataset(name=uc_table_name)\n",
        "        print(f\"Found existing dataset: {uc_table_name}\")\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        # Dataset doesn't exist, create it\n",
        "        print(f\"Dataset not found, creating: {uc_table_name}\")\n",
        "        dataset = create_dataset(uc_table_name=uc_table_name, **kwargs)\n",
        "        return dataset\n",
        "\n",
        "# Define test cases as list of dicts (MLflow 3.6+ format)\n",
        "test_cases = [\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What are the key financial highlights from the annual report?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should mention specific financial metrics from the annual report.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What are the main risks mentioned in the report?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should identify and describe key business risks.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What is the company's growth strategy?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should outline the company's strategic growth plans.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What are the major revenue sources?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should list major revenue streams and their contributions.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What sustainability initiatives are mentioned?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should describe environmental and social initiatives.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"Who are the key executives mentioned?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should name key leadership team members.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What are the future outlook and projections?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should discuss future plans and financial projections.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What challenges does the company face?\"},\n",
        "        \"expectations\": {\"expected_response\": \"The answer should identify main challenges and obstacles.\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create or get evaluation dataset in Unity Catalog\n",
        "eval_dataset = get_or_create_dataset(\n",
        "    uc_table_name=EVAL_DATASET_UC_TABLE,\n",
        ")\n",
        "\n",
        "# Add test cases to the dataset (merge_records handles duplicates)\n",
        "eval_dataset.merge_records(test_cases)\n",
        "\n",
        "print(f\"âœ“ Evaluation dataset created/updated in Unity Catalog\")\n",
        "print(f\"  Table: {EVAL_DATASET_UC_TABLE}\")\n",
        "print(f\"  Test cases: {len(test_cases)}\")\n",
        "print(\"\\nSample questions:\")\n",
        "for i, tc in enumerate(test_cases[:3], 1):\n",
        "    print(f\"{i}. {tc['inputs']['question']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Custom Scorers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "# Custom code-based scorer for response quality metrics\n",
        "@scorer\n",
        "def response_quality(inputs: dict, outputs: str) -> dict:\n",
        "    \"\"\"Calculate basic quality metrics for the response\"\"\"\n",
        "    response = str(outputs) if outputs else \"\"\n",
        "    question = inputs.get('question', '') if isinstance(inputs, dict) else str(inputs)\n",
        "    \n",
        "    return {\n",
        "        \"response_length\": len(response),\n",
        "        \"word_count\": len(response.split()),\n",
        "        \"has_answer\": len(response) > 50,\n",
        "        \"mentions_context\": \"context\" in response.lower() or \"document\" in response.lower()\n",
        "    }\n",
        "\n",
        "# Custom LLM-as-judge scorer for relevance using make_judge\n",
        "relevance_judge = make_judge(\n",
        "    name=\"relevance\",\n",
        "    instructions=(\n",
        "        \"Evaluate how relevant and accurate the answer is to the question.\\n\\n\"\n",
        "        \"Question: {{ inputs/question }}\\n\"\n",
        "        \"Answer: {{ outputs }}\\n\"\n",
        "        \"Expected Answer Context: {{ expectations/expected_response }}\\n\\n\"\n",
        "        \"Rate the relevance as:\\n\"\n",
        "        \"- 'highly_relevant': Directly answers the question with accurate information\\n\"\n",
        "        \"- 'mostly_relevant': Mostly relevant with minor tangential information\\n\"\n",
        "        \"- 'partially_relevant': Partially relevant but missing key points\\n\"\n",
        "        \"- 'minimally_relevant': Minimally relevant with significant gaps\\n\"\n",
        "        \"- 'not_relevant': Not relevant or incorrect\\n\"\n",
        "    ),\n",
        "    feedback_value_type=Literal[\n",
        "        \"highly_relevant\",\n",
        "        \"mostly_relevant\",\n",
        "        \"partially_relevant\",\n",
        "        \"minimally_relevant\",\n",
        "        \"not_relevant\"\n",
        "    ],\n",
        "    model=JUDGE_LLM,\n",
        ")\n",
        "\n",
        "# Custom LLM-as-judge scorer for completeness using make_judge\n",
        "completeness_judge = make_judge(\n",
        "    name=\"completeness\",\n",
        "    instructions=(\n",
        "        \"Evaluate how complete and comprehensive the answer is.\\n\\n\"\n",
        "        \"Question: {{ inputs/question }}\\n\"\n",
        "        \"Answer: {{ outputs }}\\n\"\n",
        "        \"Expected Answer Context: {{ expectations/expected_response }}\\n\\n\"\n",
        "        \"Rate the completeness as:\\n\"\n",
        "        \"- 'fully_complete': Fully comprehensive, covers all important aspects\\n\"\n",
        "        \"- 'mostly_complete': Mostly complete with minor gaps\\n\"\n",
        "        \"- 'partially_complete': Partially complete, missing some key information\\n\"\n",
        "        \"- 'incomplete': Incomplete, significant information missing\\n\"\n",
        "        \"- 'very_incomplete': Very incomplete or lacks substance\\n\"\n",
        "    ),\n",
        "    feedback_value_type=Literal[\n",
        "        \"fully_complete\",\n",
        "        \"mostly_complete\",\n",
        "        \"partially_complete\",\n",
        "        \"incomplete\",\n",
        "        \"very_incomplete\"\n",
        "    ],\n",
        "    model=JUDGE_LLM,\n",
        ")\n",
        "\n",
        "# Register all scorers to the experiment (MLflow 3.6+)\n",
        "# Each scorer gets a unique name for tracking and retrieval\n",
        "\n",
        "# Built-in scorers with registration\n",
        "safety_scorer = Safety().register(name=\"safety_judge\")\n",
        "relevance_to_query_scorer = RelevanceToQuery().register(name=\"relevance_to_query_judge\")\n",
        "\n",
        "# Register custom code-based scorer\n",
        "response_quality_scorer = response_quality.register(name=\"response_quality_metrics\")\n",
        "\n",
        "# Register LLM-as-judge scorers\n",
        "relevance_judge_registered = relevance_judge.register(name=\"custom_relevance_judge\")\n",
        "completeness_judge_registered = completeness_judge.register(name=\"custom_completeness_judge\")\n",
        "\n",
        "# Combine all registered scorers for evaluation\n",
        "all_scorers = [\n",
        "    safety_scorer,\n",
        "    relevance_to_query_scorer,\n",
        "    response_quality_scorer,\n",
        "    relevance_judge_registered,\n",
        "    completeness_judge_registered\n",
        "]\n",
        "\n",
        "print(\"âœ“ Scorers defined and registered to experiment:\")\n",
        "print(\"  Built-in (registered):\")\n",
        "print(\"    - safety_judge (Safety)\")\n",
        "print(\"    - relevance_to_query_judge (RelevanceToQuery)\")\n",
        "print(\"  Code-based (registered):\")\n",
        "print(\"    - response_quality_metrics\")\n",
        "print(\"  LLM Judges (registered):\")\n",
        "print(\"    - custom_relevance_judge\")\n",
        "print(\"    - custom_completeness_judge\")\n",
        "\n",
        "# List all registered scorers in the experiment\n",
        "print(\"\\nâœ“ Registered scorers in experiment:\")\n",
        "for s in list_scorers():\n",
        "    print(f\"  - {s.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run MLflow GenAI Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View aggregate metrics from the evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"AGGREGATE METRICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "metrics_df = pd.DataFrame([results.metrics]).T\n",
        "metrics_df.columns = [\"Value\"]\n",
        "metrics_df.index.name = \"Metric\"\n",
        "display(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View per-row evaluation results\n",
        "print(\"=\" * 60)\n",
        "print(\"PER-ROW EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_results_df = results.eval_results\n",
        "\n",
        "# Show key columns for readability\n",
        "display_cols = [col for col in eval_results_df.columns if col not in ['trace']]\n",
        "display(eval_results_df[display_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run MLflow GenAI Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze LLM judge scores distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Relevance score distribution\n",
        "if 'relevance' in eval_results_df.columns:\n",
        "    relevance_counts = eval_results_df['relevance'].value_counts()\n",
        "    axes[0].bar(relevance_counts.index, relevance_counts.values, color='steelblue')\n",
        "    axes[0].set_title('Relevance Score Distribution', fontsize=12)\n",
        "    axes[0].set_xlabel('Relevance Rating')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Completeness score distribution\n",
        "if 'completeness' in eval_results_df.columns:\n",
        "    completeness_counts = eval_results_df['completeness'].value_counts()\n",
        "    axes[1].bar(completeness_counts.index, completeness_counts.values, color='darkorange')\n",
        "    axes[1].set_title('Completeness Score Distribution', fontsize=12)\n",
        "    axes[1].set_xlabel('Completeness Rating')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze response quality metrics\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Response length distribution\n",
        "if 'response_quality/response_length' in eval_results_df.columns:\n",
        "    lengths = eval_results_df['response_quality/response_length']\n",
        "    axes[0].hist(lengths, bins=10, color='seagreen', edgecolor='black')\n",
        "    axes[0].axvline(lengths.mean(), color='red', linestyle='--', label=f'Mean: {lengths.mean():.0f}')\n",
        "    axes[0].set_title('Response Length Distribution', fontsize=12)\n",
        "    axes[0].set_xlabel('Character Count')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].legend()\n",
        "\n",
        "# Word count distribution\n",
        "if 'response_quality/word_count' in eval_results_df.columns:\n",
        "    word_counts = eval_results_df['response_quality/word_count']\n",
        "    axes[1].hist(word_counts, bins=10, color='mediumpurple', edgecolor='black')\n",
        "    axes[1].axvline(word_counts.mean(), color='red', linestyle='--', label=f'Mean: {word_counts.mean():.0f}')\n",
        "    axes[1].set_title('Word Count Distribution', fontsize=12)\n",
        "    axes[1].set_xlabel('Word Count')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary insights\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION INSIGHTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Safety analysis\n",
        "if 'Safety/safe' in eval_results_df.columns:\n",
        "    safe_pct = (eval_results_df['Safety/safe'].sum() / len(eval_results_df)) * 100\n",
        "    print(f\"\\nðŸ›¡ï¸  Safety: {safe_pct:.1f}% of responses are safe\")\n",
        "\n",
        "# Relevance analysis\n",
        "if 'RelevanceToQuery/relevant_to_query' in eval_results_df.columns:\n",
        "    relevant_pct = (eval_results_df['RelevanceToQuery/relevant_to_query'].sum() / len(eval_results_df)) * 100\n",
        "    print(f\"ðŸŽ¯ Query Relevance: {relevant_pct:.1f}% of responses are relevant\")\n",
        "\n",
        "# Custom relevance judge analysis\n",
        "if 'relevance' in eval_results_df.columns:\n",
        "    high_relevance = eval_results_df['relevance'].isin(['highly_relevant', 'mostly_relevant']).sum()\n",
        "    high_rel_pct = (high_relevance / len(eval_results_df)) * 100\n",
        "    print(f\"ðŸ“Š High Relevance (Judge): {high_rel_pct:.1f}% scored highly/mostly relevant\")\n",
        "\n",
        "# Completeness analysis\n",
        "if 'completeness' in eval_results_df.columns:\n",
        "    high_complete = eval_results_df['completeness'].isin(['fully_complete', 'mostly_complete']).sum()\n",
        "    high_comp_pct = (high_complete / len(eval_results_df)) * 100\n",
        "    print(f\"âœ… High Completeness (Judge): {high_comp_pct:.1f}% scored fully/mostly complete\")\n",
        "\n",
        "# Response quality metrics\n",
        "if 'response_quality/word_count' in eval_results_df.columns:\n",
        "    avg_words = eval_results_df['response_quality/word_count'].mean()\n",
        "    print(f\"\\nðŸ“ Average Response Length: {avg_words:.0f} words\")\n",
        "\n",
        "if 'response_quality/has_answer' in eval_results_df.columns:\n",
        "    has_answer_pct = (eval_results_df['response_quality/has_answer'].sum() / len(eval_results_df)) * 100\n",
        "    print(f\"ðŸ’¬ Substantive Answers: {has_answer_pct:.1f}% have substantive responses\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify areas for improvement\n",
        "print(\"=\" * 60)\n",
        "print(\"AREAS FOR IMPROVEMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Find low-performing questions based on relevance and completeness judges\n",
        "low_performers = []\n",
        "\n",
        "if 'relevance' in eval_results_df.columns and 'completeness' in eval_results_df.columns:\n",
        "    for idx, row in eval_results_df.iterrows():\n",
        "        issues = []\n",
        "        if row.get('relevance') in ['partially_relevant', 'minimally_relevant', 'not_relevant']:\n",
        "            issues.append(f\"Low relevance: {row['relevance']}\")\n",
        "        if row.get('completeness') in ['partially_complete', 'incomplete', 'very_incomplete']:\n",
        "            issues.append(f\"Low completeness: {row['completeness']}\")\n",
        "        \n",
        "        if issues:\n",
        "            question = row['inputs'].get('question', 'Unknown') if isinstance(row['inputs'], dict) else str(row['inputs'])\n",
        "            low_performers.append({\n",
        "                'question': question[:80] + '...' if len(question) > 80 else question,\n",
        "                'issues': ', '.join(issues)\n",
        "            })\n",
        "\n",
        "if low_performers:\n",
        "    print(\"\\nâš ï¸  Questions that need attention:\\n\")\n",
        "    for i, item in enumerate(low_performers, 1):\n",
        "        print(f\"{i}. {item['question']}\")\n",
        "        print(f\"   Issues: {item['issues']}\\n\")\n",
        "else:\n",
        "    print(\"\\nâœ… All questions performed well across relevance and completeness metrics!\")\n",
        "\n",
        "# Overall recommendation\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"RECOMMENDATIONS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "recommendations = []\n",
        "if 'relevance' in eval_results_df.columns:\n",
        "    low_rel = eval_results_df['relevance'].isin(['partially_relevant', 'minimally_relevant', 'not_relevant']).sum()\n",
        "    if low_rel > len(eval_results_df) * 0.3:\n",
        "        recommendations.append(\"â€¢ Consider improving retrieval quality - many responses lack relevance\")\n",
        "\n",
        "if 'completeness' in eval_results_df.columns:\n",
        "    low_comp = eval_results_df['completeness'].isin(['partially_complete', 'incomplete', 'very_incomplete']).sum()\n",
        "    if low_comp > len(eval_results_df) * 0.3:\n",
        "        recommendations.append(\"â€¢ Responses may need more context - consider increasing TOP_K_RESULTS\")\n",
        "\n",
        "if 'response_quality/word_count' in eval_results_df.columns:\n",
        "    avg_words = eval_results_df['response_quality/word_count'].mean()\n",
        "    if avg_words < 50:\n",
        "        recommendations.append(\"â€¢ Responses are quite short - consider adjusting prompts for more detailed answers\")\n",
        "\n",
        "if not recommendations:\n",
        "    recommendations.append(\"â€¢ Model is performing well overall!\")\n",
        "    recommendations.append(\"â€¢ Consider adding more diverse test cases to validate robustness\")\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Link to MLflow UI for detailed exploration\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPLORE IN MLFLOW UI\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nEvaluation Run ID: {eval_run_id}\")\n",
        "print(f\"Dataset: {EVAL_DATASET_UC_TABLE}\")\n",
        "print(f\"\\nView detailed results, traces, and artifacts in the MLflow Experiment UI:\")\n",
        "print(f\"Experiment: {EVAL_EXPERIMENT_NAME}\")\n",
        "print(\"\\nThe MLflow UI provides:\")\n",
        "print(\"â€¢ Interactive trace viewer for each evaluation\")\n",
        "print(\"â€¢ Scorer feedback and rationale details\")\n",
        "print(\"â€¢ Artifact downloads and comparison tools\")\n",
        "print(\"â€¢ Registered scorers and datasets in Unity Catalog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Production Monitoring Setup (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start production monitoring with scorer sampling (MLflow 3.6+)\n",
        "# This enables continuous evaluation of production traces\n",
        "\n",
        "# Start safety scorer for production monitoring\n",
        "# Sample rate determines what percentage of traces are evaluated\n",
        "safety_monitor = safety_scorer.start(\n",
        "    sampling_config=ScorerSamplingConfig(sample_rate=PRODUCTION_SAMPLE_RATE)\n",
        ")\n",
        "\n",
        "# Start relevance scorer for production monitoring\n",
        "relevance_monitor = relevance_to_query_scorer.start(\n",
        "    sampling_config=ScorerSamplingConfig(sample_rate=PRODUCTION_SAMPLE_RATE)\n",
        ")\n",
        "\n",
        "print(\"âœ“ Production monitoring started:\")\n",
        "print(f\"  - safety_judge: {PRODUCTION_SAMPLE_RATE*100:.0f}% sample rate\")\n",
        "print(f\"  - relevance_to_query_judge: {PRODUCTION_SAMPLE_RATE*100:.0f}% sample rate\")\n",
        "print(\"\\nMonitored scorers will automatically evaluate incoming traces.\")\n",
        "print(\"View results in MLflow Experiment UI under 'Traces' tab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop production monitoring when done (cleanup)\n",
        "# Uncomment to stop the monitors:\n",
        "# safety_monitor.stop()\n",
        "# relevance_monitor.stop()\n",
        "# print(\"âœ“ Production monitoring stopped\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation using Unity Catalog dataset\n",
        "print(\"Running evaluation...\\n\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"rag_agent_evaluation\") as eval_run:\n",
        "    \n",
        "    # Load the model for prediction\n",
        "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
        "    \n",
        "    # Define prediction function\n",
        "    def predict_fn(question: str) -> str:\n",
        "        \"\"\"\n",
        "        Prediction function that takes a question string and returns a prediction.\n",
        "        MLflow will call this with: predict_fn(question='What is...')\n",
        "        \"\"\"\n",
        "        model_input = pd.DataFrame({\"question\": [question]})\n",
        "        result = loaded_model.predict(model_input)\n",
        "        return result if isinstance(result, str) else result[0]\n",
        "    \n",
        "    print(f\"Using evaluation dataset: {EVAL_DATASET_UC_TABLE}\")\n",
        "    print(f\"Scorers to run: {len(all_scorers)}\")\n",
        "    \n",
        "    # Evaluate using Unity Catalog dataset and registered scorers\n",
        "    results = evaluate(\n",
        "        data=eval_dataset,  # Use UC dataset instead of inline DataFrame\n",
        "        predict_fn=predict_fn,\n",
        "        scorers=all_scorers  # Registered scorers\n",
        "    )\n",
        "    \n",
        "    eval_run_id = eval_run.info.run_id\n",
        "    \n",
        "    print(f\"\\nâœ“ Evaluation completed!\")\n",
        "    print(f\"Evaluation Run ID: {eval_run_id}\")\n",
        "    print(f\"Dataset: {EVAL_DATASET_UC_TABLE}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
